# EmoCaliber

Project page of:
**EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration**
*Daiqing Wu, Dongbao Yang, Can Ma, Yu Zhou*

## ðŸ“– Overview  

This repository provides the **code and data** introduced in our [paper](https://arxiv.org/abs/2512.15528), consisting of five key components:

- **EmoCaliber**: A confidence-calibrated MLLM for Visual Emotion Comprehension (VEC). Link: [https://huggingface.co/datasets/wudq/VECBench](https://huggingface.co/datasets/wudq/VECBench)
- **VECBench**: A unified benchmark for VEC that comprises six popular datasets. Link: [https://huggingface.co/datasets/wudq/VECBench](https://huggingface.co/datasets/wudq/VECBench)
- **VEC-CoT**: A dataset with high-quality image-label-CoT triplets. Link: [https://huggingface.co/datasets/wudq/VECBench/tree/main/VEC-CoT](https://huggingface.co/datasets/wudq/VECBench/tree/main/VEC-CoT)
- **Training Script**: The scripts we used to train EmoCaliber based on ms-swift (for stage 1,2) and verl (stage 3). 
- **Evaluation Code**: Code for performing inference and evaluation on VECBench.
