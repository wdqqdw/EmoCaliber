# EmoCaliber

Project page of:
**EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration**
*Daiqing Wu, Dongbao Yang, Can Ma, Yu Zhou*

## ðŸ“– Overview  

This repository provides the **code and data** introduced in our paper, consisting of five key components:

- **EmoCaliber**: A confidence-calibrated MLLM for Visual Emotion Comprehension (VEC). Link: [![EmoCaliber](https://img.shields.io/badge/EmoCaliber-HuggingFace-orange)](https://huggingface.co/wudq/EmoCaliber)
- **VECBench**: A unified benchmark for VEC that comprises six popular datasets. Link: Link: [![VECBench](https://img.shields.io/badge/VECBench-HuggingFace-orange)](https://huggingface.co/datasets/wudq/VECBench)
- **VEC-CoT**: A dataset with high-quality image-label-CoT triplets. Link: Link: [![VECBench](https://img.shields.io/badge/VECBench-HuggingFace-orange)](https://huggingface.co/datasets/wudq/VECBench)
- **Training Script**: The scripts we used to train EmoCaliber based on ms-swift (for stage 1,2) and verl (stage 3). 
- **Evaluation Code**: Code for performing inference and evaluation on VECBench.
